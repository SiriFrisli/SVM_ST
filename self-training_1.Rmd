---
title: "Self-training classification round 1"
author: "Siri Frisli"
output:
  html_notebook:
    toc: yes
    number_sections: yes
    toc_depth: '3'
    toc_float:
      collapsed: no
    df_print: paged
---

```{r setup, include = FALSE}
packages <- c("tidyverse", "tidymodels", "readxl", "ROSE", "recipes", "textrecipes",
              "e1071", "caret", "glmnet", "spacyr", "textdata", "xgboost")
 
for (package in packages) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package,
                     dependencies = TRUE,
                     repos='http://cran.us.r-project.org')
  }
}

for (package in packages){
  library(package, character.only = TRUE)
}

knitr::opts_chunk$set(echo = TRUE)
```

Loading the data
```{r}
covid <- read_xlsx("E:/Data/Training samples/misinformation_labeled.xlsx")

stopwords <- read_xlsx("~/INORK/Processing/stopwords.xlsx")
custom_words <- stopwords |>
  pull(word)

covid <- covid |>
  select(tweet = text, label, id)

covid$label <- case_when(
  covid$label == 0 ~ "non.misinfo",
  covid$label == 1 ~ "misinfo"
)
```

```{r}
removeURL <- function(tweet) {
  return(gsub("http\\S+", "", tweet))
}

removeUsernames <- function(tweet) {
  return(gsub("@[a-z,A-Z,_]*[0-9]*[a-z,A-Z,_]*[0-9]*", "", tweet))
}

covid$tweet <- apply(covid["tweet"], 1, removeURL)
covid$tweet <- apply(covid["tweet"], 1, removeUsernames)

covid$label <- as.factor(covid$label) # Outcome variable needs to be factor
covid$tweet <- tolower(covid$tweet)
covid$tweet <- gsub("[[:punct:]]", " ", covid$tweet)
# covid$tweet <- str_replace_all(covid$tweet, "[0-9]", "")
```

Creating the splits:
```{r}
set.seed(1234)
covid_split <- initial_split(covid, prop = 0.8, strata = label)
train <- training(covid_split)
test <- testing(covid_split)
```

Initializing spacyr environment and loading the pre-trained word embeddings (norwegian bokmÃ¥l)
```{r}
spacy_initialize(model = "nb_core_news_sm")
no_we <- fread("~/SVM_ST/model.txt", 
                    skip = 1, header = FALSE, sep = " ", quote = "", encoding = "UTF-8")
no_we <- no_we |>
  as_tibble()
```

# Recipe
```{r}
covid_recipe <- recipe(label~tweet, data = train) |>
  step_tokenize(tweet, engine = "spacyr") |>
  step_stopwords(tweet, language = "no", keep = FALSE, 
                 stopword_source = "snowball", 
                 custom_stopword_source = custom_words) |>
  step_lemma(tweet) |>
  step_word_embeddings(tweet, embeddings = no_we) |>
  step_normalize(all_numeric_predictors()) |>
  prep()

new_train <- bake(covid_recipe, new_data = train)
new_test <- bake(covid_recipe, new_data = test)
```

Calculating the class weights, with the inverse class frequency method
```{r}
841/(table(train$label)[1] * 2)
841/(table(train$label)[2] * 2)
```

# First SVM
```{r}
set.seed(1234)
svm_model <- svm(formula = label~.,
                 data = new_train,
                 type = "C-classification",
                 kernel = "radial",
                 cross = 5,
                 class.weights = c("misinfo" = 6.570312, "non.misinfo" = 0.541184),
                 probability = TRUE)
```

```{r}
model_svm <- new_test |>
  bind_cols(predict(svm_model, new_test))

cm_svm <- confusionMatrix(table(new_test$label, model_svm$...302)) 
cm_svm$byClass["F1"] # 0.2857143    
cm_svm$byClass["Precision"] # 0.3333333    
cm_svm$byClass["Recall"] # 0.25    
```

# Loading and preprocessing full dataset
```{r}
covid_df <- readRDS("E:/Data/covid_relevant_nort_domain.RDS")

match <- subset(covid, (covid$id %in% covid_df$id))
covid_df <- covid_df |>
  anti_join(match, by = "id") |>
  rename(tweet = text)
```

```{r}
covid_df <- covid_df |>
  sample_n(50000, seed = 1234)
```

```{r}
removeURL <- function(tweet) {
  return(gsub("http\\S+", "", tweet))
}

removeUsernames <- function(tweet) {
  return(gsub("@[a-z,A-Z,_]*[0-9]*[a-z,A-Z,_]*[0-9]*", "", tweet))
}

covid_df$tweet <- apply(covid_df["tweet"], 1, removeURL)
covid_df$tweet <- apply(covid_df["tweet"], 1, removeUsernames)

covid_df$tweet <- tolower(covid_df$tweet)
covid_df$tweet <- gsub("[[:punct:]]", " ", covid_df$tweet)
# covid_df$tweet <- str_replace_all(covid_df$tweet, "[0-9]", "")
```

## SVM on the new sample
```{r}
covid_df_baked <- bake(covid_recipe, new_data = covid_df)

covid_df_pred <- predict(svm_model, covid_df_baked, probability = TRUE)
covid_df_pred_probs <- attr(covid_df_pred, "probabilities")
covid_df_pred_probs <- covid_df_pred_probs |>
  as.data.frame()
```

```{r}
covid_pred_df <- bind_cols(covid_df, covid_df_pred_probs)
covid_pred_df <- covid_pred_df |>
  filter(non.misinfo > 0.9 | misinfo > 0.9)
```

```{r}
covid_pred_df_label <- covid_pred_df |>
  mutate(label = case_when(
    misinfo > 0.9 ~ "misinfo",
    non.misinfo > 0.9 ~ "non.misinfo"
  ))
```

```{r}
covid_pred_df_label <- covid_pred_df_label |>
  select(tweet, label, id, conversation_id)
```

```{r}
covid_pred_df_label |>
  count(label)
```

Merging the two datasets and saving it as a new training sample
```{r}
covid_predicted <- full_join(covid, covid_pred_df_label)

covid_predicted |>
  count(label)
```

```{r}
saveRDS(covid_predicted, "D:/Data/Training samples/misinformation_class_1.RDS")
```

