---
title: "Case weight testing"
author: "Siri Frisli"
output:
  html_notebook:
    toc: yes
    number_sections: yes
    toc_depth: '3'
    toc_float:
      collapsed: no
    df_print: paged
---

```{r setup, include = FALSE}
packages <- c("tidyverse", "tidymodels", "readxl", "ROSE", "recipes", "textrecipes",
              "e1071", "caret", "glmnet", "spacyr", "textdata")
 
for (package in packages) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package,
                     dependencies = TRUE,
                     repos='http://cran.us.r-project.org')
  }
}

for (package in packages){
  library(package, character.only = TRUE)
}

knitr::opts_chunk$set(echo = TRUE)
```

# Loading and preprocessing data
```{r}
covid <- read_xlsx("D:/Data/Training samples/misinformation_labeled.xlsx")

stopwords <- read_xlsx("~/INORK/INORK_R/Processing/stopwords.xlsx")
custom_words <- stopwords |>
  pull(word)

covid <- covid |>
  select(tweet = text, label, id)

covid$label <- case_when(
  covid$label == 0 ~ "non.misinfo",
  covid$label == 1 ~ "misinfo"
)
```

```{r}
removeURL <- function(tweet) {
  return(gsub("http\\S+", "", tweet))
}

removeUsernames <- function(tweet) {
  return(gsub("@[a-z,A-Z,_]*[0-9]*[a-z,A-Z,_]*[0-9]*", "", tweet))
}

covid$tweet <- apply(covid["tweet"], 1, removeURL)
covid$tweet <- apply(covid["tweet"], 1, removeUsernames)

covid$label <- as.factor(covid$label) # Outcome variable needs to be factor
covid$tweet <- tolower(covid$tweet)
covid$tweet <- gsub("[[:punct:]]", " ", covid$tweet)
# covid$tweet <- str_replace_all(covid$tweet, "[0-9]", "")
```

Splitting the data
```{r}
set.seed(1234)
covid_split <- initial_split(covid, prop = 0.8, strata = label)
train <- training(covid_split)
test <- testing(covid_split)
```

Checking the class imbalance in the training data:
```{r}
train |>
  count(label) 
```

Initializing spacyr environment and loading the pre-trained word embeddings
```{r}
spacy_initialize(model = "nb_core_news_sm")
no_we <- read.table("C:/Users/sirifris.ADA/OneDrive - OsloMet/Dokumenter/no.wiki.bpe.vs50000.d200.w2v.txt", 
                    header = FALSE, sep = " ", quote = "", encoding = "UTF-8")
no_we <- no_we |>
  as_tibble()
no_we$V1 <- gsub("▁", "", no_we$V1)
```

# Creating the recipe

Adding weights
```{r}
train <- train |>
  mutate(case_wts = ifelse(label == "misinfo", 8.2,0.6),
         case_wts = importance_weights(case_wts))
```

5-fold cross validation
```{r}
set.seed(8008)
folds <- vfold_cv(train, strata = label, v = 5, repeats = 2)
```

Log rec spec 
```{r}
lr_spec <- logistic_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")
```

The recipe
```{r}
covid_recipe <- recipe(label~tweet+case_wts, data = train) |>
  step_tokenize(tweet, engine = "spacyr") |>
  step_stopwords(tweet, language = "no", keep = FALSE, 
                 stopword_source = "snowball", 
                 custom_stopword_source = custom_words) |>
  step_lemma(tweet) |>
  step_word_embeddings(tweet, embeddings = no_we) |>
  step_ns(starts_with("non_linear"), deg_free = 10) |>
  step_normalize(all_numeric_predictors())

covid_recipe
```
```{r}
lr_workf <- workflow() |>
  add_model(lr_spec) |>
  add_recipe(covid_recipe) |>
  add_case_weights(case_wts)
lr_workf
```
```{r}
cls_metric <- metric_set(yardstick::precision, yardstick::recall, yardstick::f_meas)
grid <- tibble(penalty = 10^seq(-3, 0, length.out = 20))
```

```{r}
set.seed(1)
lr_res <- lr_workf |>
  tune_grid(resamples = folds, grid = grid, metrics = cls_metric)

autoplot(lr_res)
```

```{r}
lr_unwt_workf <- lr_workf |>
  remove_case_weights()
```

```{r}
set.seed(3) 
lr_unwt_res <- lr_unwt_workf |>
  tune_grid(resamples = folds, grid = grid, metrics = cls_metric)

autoplot(lr_unwt_res)
```
The importance weights seems to do their job, as the balance between the metrics is much better in the weighted analysis 
